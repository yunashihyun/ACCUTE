import openpyxl
from PyPDF2 import PdfReader
import concurrent.futures
import json
from typing import List
import cohere
from qdrant_client import QdrantClient
from qdrant_client.http import models as qdrant_models
from openai import OpenAI  # openai==1.52.2
import re

# ===================== 0. ì„¤ì • =====================
# (OpenAI, Cohere, Qdrant ë“± ê°ì¢… í‚¤ì™€ í˜¸ìŠ¤íŠ¸ ì •ë³´ëŠ” ì‹¤ì œ ê°’ìœ¼ë¡œ êµì²´í•´ ì£¼ì„¸ìš”.)
upstage_api_key = "up_6qo5o2ZeW3LLhCxybWXDjdUAeIHtC"
cohere_api_key = "WwcsB55oeO2h9mdBruglQe6chqNYmICur98HPmET"
qdrant_api_key = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.wZ2KnWo8fU2lisqUxP2t0RVO2eKVgUzPenwImNtkUXg"
qdrant_host = "a083dbb5-2a05-4572-a44c-85ce96be6123.us-east4-0.gcp.cloud.qdrant.io"
COLLECTION_NAME = "please_work"

# ===================== 0-1. ì—‘ì…€ ì²˜ë¦¬ (ì˜ˆì‹œ) =====================
workbook = openpyxl.load_workbook('crawled_data.xlsx')  # íŒŒì¼ëª…, ê²½ë¡œ ì¡°ì •
sheet = workbook.active  # í˜¹ì€ workbook['ì‹œíŠ¸ëª…']

law_list = []
for cell in sheet['C']:
    value = str(cell.value) if cell.value is not None else ""
    law_list.append(value)

# ===================== 0-2. í´ë¼ì´ì–¸íŠ¸ ì„¸íŒ… =====================
llm_client = OpenAI(
    api_key=upstage_api_key,
    base_url="https://api.upstage.ai/v1"
)
co = cohere.Client(cohere_api_key)
qdrant_client = QdrantClient(
    host=qdrant_host,
    port=6333,
    https=True,
    api_key=qdrant_api_key
)

# ===================== 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ =====================
def extract_text_from_pdf(pdf_path: str) -> str:
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    return text

# ===================== 2. í…ìŠ¤íŠ¸ ë¶„í•  (ê¸°ë³¸: 8,000ì) =====================
def split_text_by_length(text: str, chunk_size: int = 8000) -> list:
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

# ===================== 3. ê°œë³„ chunk ìš”ì•½ í•¨ìˆ˜ =====================
def summarize_chunk(chunk: str) -> str:
    prompt = f"""ë‹¤ìŒì€ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œì˜ ì¼ë¶€ì…ë‹ˆë‹¤.

ì´ ë³´ê³ ì„œì—ì„œ ì•„ë˜ í•­ëª©ë“¤ì— í•´ë‹¹í•˜ëŠ” ì¤‘ìš”í•œ íšŒê³„ ì •ë³´ê°€ ìˆë‹¤ë©´ ê°€ëŠ¥í•œ í•œ ëª¨ë‘ í¬í•¨í•˜ì—¬ ìš”ì•½í•´ ì£¼ì„¸ìš”.

- íšŒê³„ì •ì±…ë³€ê²½ (íšŒê³„ì •ì±… ë˜ëŠ” íšŒê³„ì¶”ì • ë³€ê²½ ë‚´ìš©)
- ìˆ˜ìµì¸ì‹_ê¸°ì¤€ (ìˆ˜ìµì„ ì–´ë–¤ ë°©ì‹/ì‹œì ìœ¼ë¡œ ì¸ì‹í•˜ëŠ”ì§€)
- ì¡°ê±´ë¶€ìˆ˜ìµ_ë³€ìˆ˜ë³´ìƒ (ì„±ê³¼ê¸‰ ë“± ì¡°ê±´ë¶€ ìˆ˜ìµ ê´€ë ¨ ì²˜ë¦¬)
- ê³„ì•½ë³€ê²½ (ê³„ì•½ ì¡°ê±´ ë³€ê²½ê³¼ íšŒê³„ì˜í–¥)
- ì§„í–‰ë¥ ê¸°ì¤€ìˆ˜ìµ (ì¥ê¸°ê³µì‚¬ë‚˜ ìš©ì—­ì— ëŒ€í•œ ìˆ˜ìµ ì¸ì‹)
- ë¦¬ìŠ¤ë¶€ì™¸ì²˜ë¦¬ (ìì‚°Â·ë¶€ì±„ ì¸ì‹ ì œì™¸ëœ ë¦¬ìŠ¤ ê´€ë ¨ ì‚¬ìœ )
- ì¶©ë‹¹ë¶€ì±„_ë¯¸ì¸ì‹ (ìš°ë°œì±„ë¬´ ì¡´ì¬ì—ë„ ë¶ˆêµ¬í•˜ê³  ë¯¸ì¸ì‹ëœ ì‚¬ìœ )
- ì •ë¶€ë³´ì¡°ê¸ˆì²˜ë¦¬ (ì •ë¶€ë³´ì¡°ê¸ˆ ì¸ì‹ ë° ì²˜ë¦¬ ë°©ì‹)
- ë¬´í˜•ìì‚°_ìì‚°í™”ì—¬ë¶€ (ê°œë°œë¹„ ë“± ë¬´í˜•ìì‚°ì˜ ìì‚°í™” ì—¬ë¶€)
- ì†ìƒê²€ì‚¬ (ì˜ì—…ê¶Œ, íˆ¬ììì‚° ë“±ì˜ ì†ìƒê²€ì‚¬ ì—¬ë¶€ì™€ ê¸°ì¤€)
- ê°ê°€ìƒê°ë°©ë²•_ë³€ê²½ (ê°ê°€ìƒê° ë°©ì‹ ë˜ëŠ” ë‚´ìš©ì—°ìˆ˜ ë³€ê²½ ë‚´ìš©)

ë˜í•œ ê¸°ì—…ì˜ ë°°ê²½ì„ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ë‹¤ìŒ ì •ë³´ë„ í¬í•¨í•´ ì£¼ì„¸ìš”:

- ì‚°ì—…ë¶„ë¥˜
- ì œí’ˆìœ í˜•
- ìˆ˜ìµêµ¬ì¡°
- ë§¤ì¶œêµ¬ì„±
- ê³ ê°ìœ í˜•
- ê³„ì•½êµ¬ì¡°
- ì—°ê²°ëŒ€ìƒì—¬ë¶€ (ì—°ê²° vs ê°œë³„ ì¬ë¬´ì œí‘œ ê¸°ì¤€)
- íšŒê³„ê¸°ì¤€ (K-IFRS, K-GAAP ë“±)
- ìƒì¥ì—¬ë¶€ (ìƒì¥ or ë¹„ìƒì¥)

â€» ê°€ëŠ¥í•œ í•­ëª©ì´ ë§ì€ ê²½ìš° ìš”ì•½ì´ ê¸¸ì–´ì ¸ë„ ê´œì°®ìŠµë‹ˆë‹¤.
â€» í•­ëª©ì´ ì—†ìœ¼ë©´ ìƒëµí•´ë„ ë©ë‹ˆë‹¤.

\n\n{chunk}"""
    try:
        response = llm_client.chat.completions.create(
            model="solar-pro",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ íšŒê³„ ë¬¸ì„œë¥¼ ìš”ì•½í•  ë•Œ, í•µì‹¬ íšŒê³„ ì´ìŠˆë¥¼ ë†“ì¹˜ì§€ ì•ŠëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            stream=False
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[ìš”ì•½ ì‹¤íŒ¨] {e}")
        return "[ìš”ì•½ ì‹¤íŒ¨]"

# ===================== 4. ë³‘ë ¬ ìš”ì•½ ì²˜ë¦¬ =====================
def summarize_in_chunks_parallel(text: str, max_workers: int = 6) -> list:
    chunks = split_text_by_length(text)
    print(f"[âš¡] {len(chunks)}ê°œì˜ chunkë¥¼ ë³‘ë ¬ë¡œ ìš”ì•½ ì¤‘...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        summaries = list(executor.map(summarize_chunk, chunks))
    return summaries

# ===================== 5. í†µí•© ìš”ì•½ =====================
def merge_summaries(summaries: list) -> str:
    combined = "\n\n".join(summaries)
    prompt = f"""ë‹¤ìŒì€ íšŒê³„ ë³´ê³ ì„œë¥¼ ë‚˜ëˆ ì„œ ìš”ì•½í•œ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ë“¤ì„ í•˜ë‚˜ì˜ ìµœì¢… ìš”ì•½ìœ¼ë¡œ í†µí•©í•´ ì£¼ì„¸ìš”:\n\n{combined}"""
    try:
        response = llm_client.chat.completions.create(
            model="solar-pro",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” íšŒê³„ ë³´ê³ ì„œë¥¼ í†µí•© ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì•¼."},
                {"role": "user", "content": prompt}
            ],
            stream=False
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[í†µí•© ìš”ì•½ ì‹¤íŒ¨] {e}")
        return "[í†µí•© ìš”ì•½ ì‹¤íŒ¨]"

# ===================== 6. ì „ì²´ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ =====================
def summarize_pdf_fully(pdf_path: str) -> str:
    print(f"[ğŸ“„] PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {pdf_path}")
    raw_text = extract_text_from_pdf(pdf_path)
    print(f"[âœ‚ï¸] í…ìŠ¤íŠ¸ ë¶„í•  + ë³‘ë ¬ ìš”ì•½ ì¤‘...")
    chunk_summaries = summarize_in_chunks_parallel(raw_text, max_workers=6)
    print(f"[ğŸ§ ] í†µí•© ìš”ì•½ ìƒì„± ì¤‘...")
    final_summary = merge_summaries(chunk_summaries)
    return final_summary

# ===================== 7. RAG AGI ì˜ˆì‹œ íŒŒì´í”„ë¼ì¸ (TASK ë¶„ë¥˜ & ì²˜ë¦¬) =====================
def classify_task_with_llm(task_prompt: str) -> str:
    classification_prompt = f"""
ìš°ë¦¬ê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” TASKëŠ” ì•„ë˜ ë‘ ê°€ì§€ì…ë‹ˆë‹¤:

1) "ìœ ì‚¬í•œ ë‹¤ë¥¸ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œ ê²€ìƒ‰"
2) "ì…ë ¥ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œì—ì„œ ì‚¬ìš©ëœ ë²• ì°¾ì•„ì£¼ê¸°"

ì‚¬ìš©ìì˜ ìš”ì²­: {task_prompt}

ìœ„ ìš”ì²­ì´ 1ë²ˆ, 2ë²ˆ ì¤‘ ì–´ë””ì— ê°€ì¥ ì˜ í•´ë‹¹í•˜ë‚˜ìš”?
- 1 or 2 ë¡œë§Œ ë‹µí•˜ì„¸ìš”.
- ë‘˜ ë‹¤ ì•„ë‹ˆë©´ "None"ì´ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”.
"""
    response = llm_client.chat.completions.create(
        model="solar-pro",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë¶„ë¥˜ë¥¼ ë„ì™€ì£¼ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": classification_prompt}
        ],
        stream=False
    )
    classification = response.choices[0].message.content.strip()
    if classification not in ["1", "2"]:
        classification = "None"
    return classification

def retrieve_qdrant_docs_by_ids(qdrant_client, collection_name: str, doc_ids: list):
    retrieved = qdrant_client.retrieve(
        collection_name=collection_name,
        ids=doc_ids
    )
    return retrieved

def search_and_rerank_with_indexing(qdrant_client, co, query_vector, collection_name):
    results = qdrant_client.search(
        collection_name=collection_name,
        query_vector=query_vector,
        limit=5
    )
    candidates = []
    for r in results:
        candidates.append({
            "id": r.id,
            "score": r.score,
            "payload": r.payload
        })

    documents = [c["payload"]["text"] for c in candidates]
    rerank_response = co.rerank(
        query="ìœ ì‚¬í•œ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œ ì°¾ê¸°",
        documents=documents,
        top_n=len(documents),
        model="rerank-multilingual-v3.0"
    )
    reranked_indices = sorted(
        rerank_response.results,
        key=lambda x: x.relevance_score,
        reverse=True
    )

    reranked_candidates = []
    for i, r in enumerate(reranked_indices):
        original_idx = r.index
        reranked_candidates.append({
            "report_index": i,
            "id": candidates[original_idx]["id"],
            "score": candidates[original_idx]["score"],
            "payload": candidates[original_idx]["payload"]
        })
    return reranked_candidates

def retrieve_by_report_index(report_index: int, reranked_candidates, qdrant_client, collection_name):
    match_candidates = [c for c in reranked_candidates if c["report_index"] == report_index]
    if not match_candidates:
        print(f"[ì—­ì¶”ì  ì‹¤íŒ¨] report_index={report_index} ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    qdrant_id = match_candidates[0]["id"]
    print(f"[ì—­ì¶”ì ] report_index={report_index} â†’ Qdrant ID={qdrant_id}")

    retrieved_docs = retrieve_qdrant_docs_by_ids(qdrant_client, collection_name, [qdrant_id])
    for doc in retrieved_docs:
        print(f"[ì—­ì¶”ì  ê²°ê³¼] ID={doc.id}, payload={doc.payload}")
        return doc  # ì²« ë²ˆì§¸ docë§Œ ë°˜í™˜
    return None

def get_solar_embedding(text: str) -> List[float]:
    response = llm_client.embeddings.create(
        input=text,
        model="embedding-query"
    )
    embedding_vector = response.data[0].embedding
    return embedding_vector

def generate_final_answer_with_llm(query: str, contexts: List[str]) -> str:
    combined_context = "\n".join(contexts)
    system_prompt = (
        "ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ íšŒê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” í•˜ë‚˜ì˜ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œì™€ ìœ ì‚¬í•œ ë³´ê³ ì„œë“¤ì˜ ìš”ì•½ì…ë‹ˆë‹¤. "
        "ì´ í…ìŠ¤íŠ¸ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì–´ë–¤ ë³´ê³ ì„œê°€ ê°€ì¥ ìœ ì‚¬í•œì§€ íŒë‹¨í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n\n"
        "ìœ ì‚¬ì„± íŒë‹¨ ê¸°ì¤€ì—ëŠ” ë‹¤ìŒ ìš”ì†Œë“¤ì„ ê³ ë ¤í•˜ì„¸ìš”:\n"
        "- íšŒê³„ê¸°ì¤€ ì ìš© (K-IFRS, K-GAAP ë“±)\n"
        "- ìˆ˜ìµ ì¸ì‹ ë°©ì‹\n"
        "- ì¶©ë‹¹ë¶€ì±„ ì²˜ë¦¬ ë°©ì‹\n"
        "- ë¬´í˜•ìì‚° ì¸ì‹ ë° ìì‚°í™” ì—¬ë¶€\n"
        "- ê°ê°€ìƒê° ë°©ë²• ë° ë‚´ìš©ì—°ìˆ˜\n\n"
        "ìµœì¢… ì¶œë ¥ì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:"
    )
    user_prompt = f"""
[ë³´ê³ ì„œ ì»¨í…ìŠ¤íŠ¸]
{combined_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ìš”ì²­]
- ì…ë ¥ ë³´ê³ ì„œì™€ ê°€ì¥ ìœ ì‚¬í•œ ë³´ê³ ì„œë¥¼ 1ê°œ ì„ íƒí•˜ê³ , ê·¸ ìœ ì‚¬í•œ ì´ìœ ë¥¼ ëª…í™•íˆ ì‘ì„±í•´ ì£¼ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
{{
  "ìœ ì‚¬í•œ_ë³´ê³ ì„œ_ID": "ë³´ê³ ì„œ 4",
  "ìœ ì‚¬í•œ_ì´ìœ ": "ë‘ ë³´ê³ ì„œëŠ” ëª¨ë‘ K-IFRSë¥¼ ì ìš©í•˜ê³  ìˆìœ¼ë©°, ìˆ˜ìµ ì¸ì‹, ì¶©ë‹¹ë¶€ì±„, ë¬´í˜•ìì‚° íšŒê³„ì²˜ë¦¬ ë°©ì‹ì´ ìœ ì‚¬í•©ë‹ˆë‹¤."
}}
"""
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    response = llm_client.chat.completions.create(
        model="solar-pro",
        messages=messages,
        stream=False,
    )
    return response.choices[0].message.content

def postprocess_final_answer_with_company_name(final_answer: str, reranked_candidates: list) -> str:
    """
    LLM ê²°ê³¼ì˜ "ìœ ì‚¬í•œ_ë³´ê³ ì„œ_ID": "ë³´ê³ ì„œ 3" â†’ ì‹¤ì œ filename ë“±ìœ¼ë¡œ ë°”ê¾¸ëŠ” í•¨ìˆ˜
    """
    try:
        parsed = json.loads(final_answer)
        report_id = parsed.get("ìœ ì‚¬í•œ_ë³´ê³ ì„œ_ID", "")  # "ë³´ê³ ì„œ 3"
        if report_id.startswith("ë³´ê³ ì„œ "):
            report_index = int(report_id.replace("ë³´ê³ ì„œ ", ""))
            for c in reranked_candidates:
                if c["report_index"] == report_index:
                    # filenameì—ì„œ [íƒˆë¡œìŠ¤]ì‚¬ì—…ë³´ê³ ì„œ(2025.03.21) ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    fname = c["payload"].get("filename", "")
                    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ [íƒˆë¡œìŠ¤]ì‚¬ì—…ë³´ê³ ì„œ(2025.03.21)ë§Œ ì¶”ì¶œ
                    match = re.search(r"^(\[.*?\].*?\(\d{4}\.\d{2}\.\d{2}\))", fname)
                    if match:
                        extracted = match.group(1)
                        parsed["ìœ ì‚¬í•œ_ë³´ê³ ì„œ_ID"] = extracted
                        # JSON ì§ë ¬í™”
                        final_answer = json.dumps(parsed, ensure_ascii=False)
                        break
    except Exception as e:
        print(f"[í›„ì²˜ë¦¬ ì˜¤ë¥˜] {e}")
    return final_answer

def rag_based_agi_pipeline(task_prompt: str, report_1: str, report_2: str = "") -> str:
    task_type = classify_task_with_llm(task_prompt)
    if task_type == "1":
        query = "ìœ ì‚¬í•œ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œ ì°¾ê¸°"
        query_vector = get_solar_embedding(query)
        reranked_candidates = search_and_rerank_with_indexing(qdrant_client, co, query_vector, COLLECTION_NAME)
        top_contexts = [item["payload"]["text"] for item in reranked_candidates[:3]]
        final_answer = generate_final_answer_with_llm(query, top_contexts)

        # íšŒì‚¬ëª… í›„ì²˜ë¦¬
        final_answer = postprocess_final_answer_with_company_name(final_answer, reranked_candidates)

        # ====== ì—¬ê¸°ì„œ {} ì œê±° ë¡œì§ ì¶”ê°€ ======
        # "Similar Case"ì—ì„œ { }ë§Œ ì—†ì• ê¸°
        final_answer = re.sub(r"[{}]", "", final_answer)

        return final_answer

    elif task_type == "2":
        combined_reports = f"[ë³´ê³ ì„œ1]\n{report_1}\n\n[ë³´ê³ ì„œ2]\n{report_2}"
        system_prompt = (
            "ë‹¹ì‹ ì€ K-IFRS ë° íšŒê³„ê¸°ì¤€ ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
            "ë‹¤ìŒì€ 2ê°œì˜ íšŒê³„ ì‚¬ì—…ë³´ê³ ì„œì…ë‹ˆë‹¤. "
            "ê° ë³´ê³ ì„œì—ì„œ ì‹¤ì œë¡œ ì–¸ê¸‰ëœ íšŒê³„ê¸°ì¤€ì„œ(K-IFRS)ì™€ ë²•ë ¹ì„ ì‹ë³„í•˜ê³ , ê·¸ ë‚´ìš©ì„ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n\n"
            "âš ï¸ ì§€ì¹¨:\n"
            "- ì‹¤ì œ ë³´ê³ ì„œì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ê¸°ì¤€ì„œ ë˜ëŠ” ë²•ë ¹ë§Œ í¬í•¨í•´ ì£¼ì„¸ìš”.\n"
            "- ê¸°ì¤€ì„œëŠ” 'ê¸°ì—…íšŒê³„ê¸°ì¤€ì„œ ì œXXXXí˜¸'ì²˜ëŸ¼ í‘œê¸°ëœ í•­ëª©ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n"
            "- ë²•ë ¹ì€ 'ë²•', 'ì‹œí–‰ë ¹', 'ê·œì •' ë“±ì˜ ì •ì‹ ëª…ì¹­ìœ¼ë¡œ ì‹ë³„í•©ë‹ˆë‹¤.\n"
            "- ê´€ë ¨ ê¸°ì¤€/ë²•ë ¹ì´ ì–¸ê¸‰ëœ ë¬¸ì¥ì€ ìµœëŒ€ 5ê°œê¹Œì§€ í¬í•¨í•´ ì£¼ì„¸ìš”.\n"
            "- ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ë©°, ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."
        )
        user_prompt = f"""
[ë³´ê³ ì„œ 1]
{report_1}

[ë³´ê³ ì„œ 2]
{report_2}

ìš”ì²­: ìœ„ ë‘ ë³´ê³ ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ JSON í˜•ì‹ì— ë§ê²Œ íšŒê³„ê¸°ì¤€ì„œ ë° ë²•ë ¹ ì •ë³´ë¥¼ ì •ë¦¬í•˜ì„¸ìš”.

json
{{
    "íšŒê³„ê¸°ì¤€ì„œ_ì ìš©": [ ... ],
    "ê´€ë ¨_ë²•ë ¹": [ ... ],
    "íšŒê³„ê¸°ì¤€_ê´€ë ¨_ë¬¸ì¥": [ ... ]
}}
"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        response = llm_client.chat.completions.create(
            model="solar-pro",
            messages=messages,
            stream=False
        )
        # {} ì œê±°
        final_answer = response.choices[0].message.content
        final_answer = re.sub(r"[{}]", "", final_answer)
        return final_answer
    else:
        return "ìš”ì²­í•œ TASKë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
